#!/usr/bin/env python3
"""
Test script for performance optimizations: selective processing and chunked healing
"""

import pandas as pd
import numpy as np
import time
from agents import DataQualityAgent, DataHealingAgent
from config import Config

def create_test_dataset(size=200000):
    """Create a test dataset with various issues for performance testing"""
    np.random.seed(42)

    # Create base data
    data = {
        'id': range(size),
        'name': [f'Customer_{i}' for i in range(size)],
        'age': np.random.normal(35, 10, size).astype(int),
        'income': np.random.normal(50000, 15000, size),
        'score': np.random.uniform(0, 100, size),
        'category': np.random.choice(['A', 'B', 'C', 'D'], size),
        'status': np.random.choice(['active', 'inactive', 'pending'], size)
    }

    df = pd.DataFrame(data)

    # Introduce realistic issues
    # Add null values (2% of data)
    null_indices = np.random.choice(size, int(size * 0.02), replace=False)
    df.loc[null_indices, 'age'] = np.nan
    df.loc[null_indices[:len(null_indices)//2], 'income'] = np.nan

    # Add duplicates (1% of data)
    duplicate_indices = np.random.choice(size, int(size * 0.01), replace=False)
    for idx in duplicate_indices:
        duplicate_row = df.iloc[idx].copy()
        df = pd.concat([df, pd.DataFrame([duplicate_row])], ignore_index=True)

    # Add outliers (1% extreme values)
    outlier_indices = np.random.choice(size, int(size * 0.01), replace=False)
    df.loc[outlier_indices, 'income'] = np.random.choice([1000000, -50000], len(outlier_indices))

    return df

def test_selective_processing():
    """Test selective row processing optimization"""
    print("ğŸ§ª Testing Selective Processing Optimization")
    print("=" * 50)

    # Create test dataset (smaller for faster testing)
    df = create_test_dataset(50000)
    print(f"ğŸ“Š Created test dataset: {len(df)} rows, {len(df.columns)} columns")

    # Initialize agent
    quality_agent = DataQualityAgent()

    # Test optimized analysis
    print("\nâ±ï¸  Testing optimized analysis...")
    start_time = time.time()
    optimized_report = quality_agent.analyze_dataset_optimized(df, "Test_Dataset")
    optimized_time = time.time() - start_time

    print(f"â±ï¸  Optimized analysis completed in {optimized_time:.2f} seconds")
    print(f"   Quality score: {optimized_report['quality_score']:.1f}/100")
    print(f"   Issues found: {len(optimized_report['issues'])}")
    print(f"   Analysis method: {optimized_report['analysis_method']}")
    print(f"   Problematic rows analyzed: {optimized_report.get('problematic_rows_analyzed', 'N/A')}")

    # Test basic analysis for comparison
    print("\nâ±ï¸  Testing basic analysis (for comparison)...")
    start_time = time.time()
    basic_report = quality_agent.analyze_dataset_basic(df, "Test_Dataset")
    basic_time = time.time() - start_time

    print(f"â±ï¸  Basic analysis completed in {basic_time:.2f} seconds")
    print(f"   Quality score: {basic_report['quality_score']:.1f}/100")
    print(f"   Issues found: {len(basic_report['issues'])}")

    # Compare performance
    speedup = basic_time / optimized_time if optimized_time > 0 else 1
    print(f"ğŸš€ Performance improvement: {speedup:.1f}x faster")
    return optimized_report, basic_report

def test_chunked_healing():
    """Test chunked healing for large datasets"""
    print("\nğŸ§ª Testing Chunked Healing Optimization")
    print("=" * 50)

    # Create test dataset (smaller for faster testing)
    df = create_test_dataset(30000)
    print(f"ğŸ“Š Created test dataset: {len(df)} rows, {len(df.columns)} columns")

    # Create a healing plan with multiple actions
    healing_plan = {
        'actions': [
            {
                'type': 'HANDLE_NULLS',
                'column': 'age',
                'recommended_method': 'FILL_MEAN'
            },
            {
                'type': 'HANDLE_NULLS',
                'column': 'income',
                'recommended_method': 'FILL_MEDIAN'
            },
            {
                'type': 'REMOVE_DUPLICATES',
                'recommended_method': 'KEEP_FIRST'
            },
            {
                'type': 'HANDLE_OUTLIERS',
                'column': 'income',
                'recommended_method': 'CAP_OUTLIERS'
            }
        ]
    }

    # Initialize agent
    healing_agent = DataHealingAgent()

    # Test chunked healing
    print("\nâ±ï¸  Testing chunked healing...")
    start_time = time.time()
    healed_df, healing_summary = healing_agent.execute_healing_actions_chunked(
        df, healing_plan, chunk_size=10000
    )
    chunked_time = time.time() - start_time

    print(f"â±ï¸  Chunked healing completed in {chunked_time:.2f} seconds")
    print(f"   Chunks processed: {healing_summary.get('total_chunks', 1)}")
    print(f"   Records affected: {healing_summary.get('total_records_affected', 0)}")
    print(f"   Final shape: {healing_summary.get('final_shape', 'N/A')}")

    # Test standard healing for comparison
    print("\nâ±ï¸  Testing standard healing (for comparison)...")
    start_time = time.time()
    healed_df_standard, healing_summary_standard = healing_agent.execute_healing_actions(
        df, healing_plan
    )
    standard_time = time.time() - start_time

    print(f"â±ï¸  Standard healing completed in {standard_time:.2f} seconds")
    print(f"   Records affected: {healing_summary_standard.get('total_records_affected', 0)}")

    # Compare performance
    speedup = standard_time / chunked_time if chunked_time > 0 else 1
    print(f"ğŸš€ Performance improvement: {speedup:.1f}x faster")
    return healing_summary, healing_summary_standard

def main():
    """Run all performance tests"""
    print("ğŸš€ Performance Optimization Test Suite")
    print("=" * 60)

    try:
        # Test selective processing
        opt_report, basic_report = test_selective_processing()

        # Test chunked healing
        chunked_summary, standard_summary = test_chunked_healing()

        # Summary
        print("\nğŸ“‹ Test Summary")
        print("=" * 30)
        print("âœ… Selective processing: Working - analyzes only problematic rows")
        print("âœ… Chunked healing: Working - processes large datasets in chunks")
        print("\nğŸ¯ Key Benefits:")
        print("   â€¢ Reduced memory usage for large datasets")
        print("   â€¢ Faster processing of datasets with few issues")
        print("   â€¢ Scalable to millions of rows")
        print("   â€¢ Maintains data quality and accuracy")

    except Exception as e:
        print(f"âŒ Test failed: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()